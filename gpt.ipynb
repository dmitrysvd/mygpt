{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from load_gpt_weights import load_params\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер для GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams, params = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = hparams['n_head']\n",
    "n_embd = hparams['n_embd']\n",
    "n_ctx = hparams['n_ctx']\n",
    "n_vocab = hparams['n_vocab']\n",
    "n_layer = hparams['n_layer']\n",
    "pprint(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_tree(d):\n",
    "    if isinstance(d, np.ndarray):\n",
    "        return list(d.shape)\n",
    "    elif isinstance(d, torch.Tensor):\n",
    "        return list(d.shape)\n",
    "    elif isinstance(d, list):\n",
    "        return [shape_tree(v) for v in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: shape_tree(v) for k, v in d.items()}\n",
    "    else:\n",
    "        ValueError(\"uh oh\")\n",
    "\n",
    "pprint(shape_tree(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gelu(x):\n",
    "#     return 0.5 * x * (1 + torch.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def attention(q, k, v):\n",
    "    d = np.sqrt(q.shape[-1])\n",
    "    x = q @ k.T / d\n",
    "\n",
    "    n = x.shape[-1]\n",
    "    casual_mask = torch.triu(torch.ones(n, n), diagonal=1) * -1e10\n",
    "    x = x + casual_mask\n",
    "    return torch.softmax(x, dim=-1) @ v\n",
    "\n",
    "\n",
    "class MultiHeadCasualSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.w_proj = nn.Linear(n_embd, n_embd)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.w_attn(x)\n",
    "        q, k, v = torch.split(x, n_embd, dim=-1)\n",
    "        qkv_heads = (\n",
    "            torch.split(q, n_embd // n_head, dim=-1),\n",
    "            torch.split(k, n_embd // n_head, dim=-1),\n",
    "            torch.split(v, n_embd // n_head, dim=-1),\n",
    "        )\n",
    "        heads_qkv = list(zip(*qkv_heads))\n",
    "\n",
    "        out_heads = [attention(q, k, v) for q, k, v in heads_qkv]\n",
    "\n",
    "        x = torch.cat(out_heads, dim=-1)\n",
    "        x = self.w_proj(x)\n",
    "        return x\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.proj = nn.Linear(4 * n_embd, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.mha = MultiHeadCasualSelfAttention()\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffn = FeedForwardNetwork()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(n_vocab, n_embd)\n",
    "        self.wpe = nn.Embedding(n_ctx, n_embd)\n",
    "        blocks = [\n",
    "            Block() for _ in range(n_layer)\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.lnorm = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_wte = self.wte(x)\n",
    "        x_wpe = self.wpe(torch.arange(x.shape[0]))\n",
    "        x = x_wte + x_wpe\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.lnorm(x)\n",
    "        x = x @ self.wte.weight.T\n",
    "        return x\n",
    "    \n",
    "    def generate(self, inputs, n_tokens_to_generate, temperature=.1):\n",
    "        assert len(inputs) + n_tokens_to_generate < n_ctx\n",
    "        inputs = inputs.clone().detach()\n",
    "        for _ in range(n_tokens_to_generate):\n",
    "            output = self(inputs)\n",
    "            logits = output[-1]\n",
    "            if temperature == 0:\n",
    "                next_id = torch.argmax(logits)\n",
    "            else:\n",
    "                logits = logits / temperature\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_id = torch.multinomial(probs, 1)\n",
    "            inputs = torch.cat([inputs, next_id.view(1)])\n",
    "            yield next_id\n",
    "\n",
    "\n",
    "x = enc.encode('Alan Turing theorized')\n",
    "x = torch.tensor(x)\n",
    "test_gpt = GPT()\n",
    "y = test_gpt(x)\n",
    "print(f'Input shape {x.shape}')\n",
    "print(f'Output shape {y.shape}')\n",
    "\n",
    "test_gpt.state_dict()\n",
    "print()\n",
    "print('Parameters:')\n",
    "pprint(shape_tree(test_gpt.state_dict()))\n",
    "del test_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем веса GPT2 в модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT()\n",
    "state_dict = {\n",
    "    'wpe.weight': params['wpe'],\n",
    "    'wte.weight': params['wte'],\n",
    "    'lnorm.weight': params['ln_f']['g'],\n",
    "    'lnorm.bias': params['ln_f']['b'],\n",
    "}\n",
    "for i, block_dict in enumerate(params['blocks']):\n",
    "    state_dict[f'blocks.{i}.mha.w_attn.bias'] = block_dict['attn']['c_attn']['b']\n",
    "    state_dict[f'blocks.{i}.mha.w_attn.weight'] = block_dict['attn']['c_attn']['w'].T\n",
    "    state_dict[f'blocks.{i}.mha.w_proj.bias'] = block_dict['attn']['c_proj']['b']\n",
    "    state_dict[f'blocks.{i}.mha.w_proj.weight'] = block_dict['attn']['c_proj']['w'].T\n",
    "\n",
    "    state_dict[f'blocks.{i}.ffn.fc.bias'] = block_dict['mlp']['c_fc']['b']\n",
    "    state_dict[f'blocks.{i}.ffn.fc.weight'] = block_dict['mlp']['c_fc']['w'].T\n",
    "    state_dict[f'blocks.{i}.ffn.proj.bias'] = block_dict['mlp']['c_proj']['b']\n",
    "    state_dict[f'blocks.{i}.ffn.proj.weight'] = block_dict['mlp']['c_proj']['w'].T\n",
    "\n",
    "    state_dict[f'blocks.{i}.ln1.bias'] = block_dict['ln_1']['b']\n",
    "    state_dict[f'blocks.{i}.ln1.weight'] = block_dict['ln_1']['g']\n",
    "    state_dict[f'blocks.{i}.ln2.bias'] = block_dict['ln_2']['b']\n",
    "    state_dict[f'blocks.{i}.ln2.weight'] = block_dict['ln_2']['g']\n",
    "state_dict = {\n",
    "    k: torch.tensor(v) for k, v in state_dict.items()\n",
    "}\n",
    "gpt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Alan Turing theorized that computers would one day become'\n",
    "x = torch.tensor(enc.encode(prompt, allowed_special={'<|endoftext|>'}))\n",
    "print(f'prompt: {prompt}\\nanswer: ', end='')\n",
    "for next_id in gpt.generate(x, 8, temperature=0.0):\n",
    "    next_token = enc.decode([next_id.item()])\n",
    "    print(next_token, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
